# üöÄ Real-time Telecom Analytics Pipeline

> –ü–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π Data Engineering –ø—Ä–æ–µ–∫—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–ª–µ–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Apache Kafka, Spark Streaming, PostgreSQL –∏ Apache Airflow.

---

## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

- [–û –ø—Ä–æ–µ–∫—Ç–µ](#–æ-–ø—Ä–æ–µ–∫—Ç–µ)
- [–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞](#–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)
- [–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫](#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π-—Å—Ç–µ–∫)
- [–ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç](#–±—ã—Å—Ç—Ä—ã–π-—Å—Ç–∞—Ä—Ç)
- [–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞](#—Å—Ç—Ä—É–∫—Ç—É—Ä–∞-–ø—Ä–æ–µ–∫—Ç–∞)
- [–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã](#–∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã-—Å–∏—Å—Ç–µ–º—ã)
- [–ü—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è](#–ø—Ä–æ–±–ª–µ–º—ã-–∏-—Ä–µ—à–µ–Ω–∏—è)
- [–î–∞–ª—å–Ω–µ–π—à–µ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ](#–¥–∞–ª—å–Ω–µ–π—à–µ–µ-—Ä–∞–∑–≤–∏—Ç–∏–µ)

---

## üéØ –û –ø—Ä–æ–µ–∫—Ç–µ

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–µ–ª–µ–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–π –∏–Ω–¥—É—Å—Ç—Ä–∏–∏.

**–û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
- ‚ö° **Real-time –æ–±—Ä–∞–±–æ—Ç–∫–∞** —Å–æ–±—ã—Ç–∏–π (–∑–≤–æ–Ω–∫–∏, SMS, –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–µ—Å—Å–∏–∏, –ø–æ–ø–æ–ª–Ω–µ–Ω–∏—è –±–∞–ª–∞–Ω—Å–∞)
- üìä **–ê–≥—Ä–µ–≥–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫** –ø–æ –º–∏–Ω—É—Ç–Ω—ã–º –æ–∫–Ω–∞–º –∏ –µ–∂–µ–¥–Ω–µ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
- üîÑ **–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è** —á–µ—Ä–µ–∑ Apache Airflow
- üê≥ **–ü–æ–ª–Ω–∞—è –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü–∏—è** –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è
- üìà **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å** –∏ –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å

**Use Case:**
–°–∏—Å—Ç–µ–º–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–æ–±—ã—Ç–∏—è –æ—Ç —Ç–µ–ª–µ–∫–æ–º-–æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ (–ø–æ —Ç–∏–ø—É Beeline Kazakhstan), –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∞–Ω–∞–ª–∏—Ç–∏–∫—É –¥–ª—è –±–∏–∑–Ω–µ—Å-—Ä–µ—à–µ–Ω–∏–π.

---

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Producer  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Kafka    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Spark Streaming ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  PostgreSQL  ‚îÇ
‚îÇ  (Python)   ‚îÇ 9092 ‚îÇ  (3 part.)  ‚îÇ 29092‚îÇ  (Real-time)    ‚îÇ 5432 ‚îÇ(real_time_   ‚îÇ
‚îÇ             ‚îÇ      ‚îÇ             ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ  metrics)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚ñ≤                                              ‚îÇ
                            ‚îÇ                                              ‚îÇ
                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                      ‚îÇ
                     ‚îÇ  Zookeeper  ‚îÇ                                      ‚îÇ
                     ‚îÇ(Coordinator)‚îÇ                                      ‚ñº
                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                                ‚îÇ  Spark Batch    ‚îÇ
                                                                ‚îÇ  (Daily stats)  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                 ‚îÇ
‚îÇ                    Airflow                                   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ  ‚îÇ PostgreSQL   ‚îÇ
‚îÇ  ‚îÇ  Scheduler   ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ DAG: Daily   ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ  Webserver   ‚îÇ   ‚îÇ  ‚îÇ (daily_stats)‚îÇ
‚îÇ  ‚îÇ              ‚îÇ   ‚îÇ  Metrics     ‚îÇ   ‚îÇ              ‚îÇ   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–ü–æ—Ç–æ–∫ –¥–∞–Ω–Ω—ã—Ö:**
1. **Producer** –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —Ç–µ–ª–µ–∫–æ–º-—Å–æ–±—ã—Ç–∏—è
2. **Kafka** –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∏ —Ö—Ä–∞–Ω–∏—Ç —Å–æ–±—ã—Ç–∏—è –≤ —Ç–æ–ø–∏–∫–µ `telecom_events`
3. **Spark Streaming** —á–∏—Ç–∞–µ—Ç –∏–∑ Kafka –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥, –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –ø–æ 1-–º–∏–Ω—É—Ç–Ω—ã–º –æ–∫–Ω–∞–º
4. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∏—à—É—Ç—Å—è –≤ **PostgreSQL** (`real_time_metrics`)
5. **Spark Batch** —Ä–∞–∑ –≤ –¥–µ–Ω—å –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –¥–Ω–µ–≤–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É (`daily_stats`)
6. **Airflow** –æ—Ä–∫–µ—Å—Ç—Ä–∏—Ä—É–µ—Ç batch –æ–±—Ä–∞–±–æ—Ç–∫—É –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é

---

## üõ†Ô∏è –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è | –í–µ—Ä—Å–∏—è | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ |
|-----------|-----------|--------|------------|
| **Message Broker** | Apache Kafka | 7.5.0 | –ë—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏—è —Å–æ–±—ã—Ç–∏–π |
| **Coordination** | Apache Zookeeper | 7.5.0 | –ö–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è Kafka |
| **Stream Processing** | Apache Spark | 3.5.5 | Real-time –æ–±—Ä–∞–±–æ—Ç–∫–∞ |
| **Batch Processing** | Apache Spark | 3.5.5 | –ï–∂–µ–¥–Ω–µ–≤–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è |
| **Database** | PostgreSQL | 15 | –•—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ |
| **Orchestration** | Apache Airflow | 2.7.3 | –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è |
| **Containerization** | Docker & Docker Compose | - | –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ |
| **Language** | Python | 3.10+ | Producer, Spark jobs |

---

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### **–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:**

- Docker Desktop 4.x+
- Docker Compose 2.x+
- Python 3.10+
- 8 GB RAM –º–∏–Ω–∏–º—É–º
- 20 GB —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ

### **–£—Å—Ç–∞–Ω–æ–≤–∫–∞:**

```bash
# 1. –ö–ª–æ–Ω–∏—Ä—É–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
git clone https://github.com/yourusername/telecom-analytics.git
cd telecom-analytics

# 2. –°–æ–∑–¥–∞–π .env —Ñ–∞–π–ª
cp .env.example .env

# 3. –ó–∞–ø—É—Å—Ç–∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É
docker-compose up -d

# 4. –ü–æ–¥–æ–∂–¥–∏ 60 —Å–µ–∫—É–Ω–¥ –ø–æ–∫–∞ –≤—Å—ë –ø–æ–¥–Ω–∏–º–µ—Ç—Å—è
sleep 60

# 5. –ü—Ä–æ–≤–µ—Ä—å —Å—Ç–∞—Ç—É—Å
docker-compose ps
```

### **–ó–∞–ø—É—Å–∫ Producer:**

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install kafka-python faker

# –ó–∞–ø—É—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–±—ã—Ç–∏–π
python producer/event_generator.py
```

### **–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã:**

```bash
# Real-time –º–µ—Ç—Ä–∏–∫–∏
docker exec -it postgres psql -U telecom_user -d telecom_db -c \
  "SELECT COUNT(*) FROM real_time_metrics;"

# Airflow UI
open http://localhost:8080
# Login: admin / admin
```

---

## üìÇ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
telecom-analytics/
‚îÇ
‚îú‚îÄ‚îÄ producer/
‚îÇ   ‚îî‚îÄ‚îÄ event_generator.py          # –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ–ª–µ–∫–æ–º-—Å–æ–±—ã—Ç–∏–π
‚îÇ
‚îú‚îÄ‚îÄ spark/
‚îÇ   ‚îú‚îÄ‚îÄ streaming_job.py            # Spark Streaming job (real-time)
‚îÇ   ‚îú‚îÄ‚îÄ batch_job.py                # Spark Batch job (daily aggregation)
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                  # –û–±—Ä–∞–∑ –¥–ª—è Spark
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt            # Python –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
‚îÇ
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ daily_metrics_dag.py        # Airflow DAG –¥–ª—è batch –æ–±—Ä–∞–±–æ—Ç–∫–∏
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ init_db.sql                 # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PostgreSQL —Å—Ö–µ–º—ã
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml              # –û—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
‚îú‚îÄ‚îÄ .env                            # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
‚îî‚îÄ‚îÄ README.md                       # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (—ç—Ç–æ—Ç —Ñ–∞–π–ª)
```

---

## üîß –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã

### **1. Producer (Event Generator)**

–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —Ç–µ–ª–µ–∫–æ–º-—Å–æ–±—ã—Ç–∏—è —Å –≤–µ—Å–∞–º–∏:

```python
# –¢–∏–ø—ã —Å–æ–±—ã—Ç–∏–π –∏ –∏—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:
- data_session (50%)      # –ò–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–µ—Å—Å–∏–∏
- call (30%)              # –ó–≤–æ–Ω–∫–∏ (incoming/outgoing)
- sms (15%)               # SMS (incoming/outgoing)
- balance_recharge (5%)   # –ü–æ–ø–æ–ª–Ω–µ–Ω–∏—è –±–∞–ª–∞–Ω—Å–∞
```

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- ‚úÖ –ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–º–µ—Ä–æ–≤ (GDPR compliance)
- ‚úÖ –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º
- ‚úÖ –ü–∏–∫–æ–≤—ã–µ —á–∞—Å—ã (—É—Ç—Ä–æ/–≤–µ—á–µ—Ä)
- ‚úÖ –°–ª—É—á–∞–π–Ω—ã–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –æ–±—ä—ë–º—ã —Ç—Ä–∞—Ñ–∏–∫–∞, —Å—É–º–º—ã

**–ó–∞–ø—É—Å–∫:**
```bash
python producer/event_generator.py
```

---

### **2. Kafka + Zookeeper**

**–¢–æ–ø–∏–∫:** `telecom_events` (3 –ø–∞—Ä—Ç–∏—Ü–∏–∏, replication factor = 1)

**–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:**
- `KAFKA_AUTO_CREATE_TOPICS_ENABLE: true` - –∞–≤—Ç–æ—Å–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–ø–∏–∫–æ–≤
- Retention: 7 –¥–Ω–µ–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
- Compression: none

**–ü–æ—Ä—Ç—ã:**
- `9092` - –¥–ª—è Producer (–≤–Ω–µ—à–Ω–∏–π –¥–æ—Å—Ç—É–ø —Å —Ö–æ—Å—Ç–∞)
- `29092` - –¥–ª—è Spark (–≤–Ω—É—Ç—Ä–∏ Docker —Å–µ—Ç–∏)
- `2181` - Zookeeper

**Healthcheck:**
```bash
kafka-topics --bootstrap-server localhost:9092 --list
```

---

### **3. Spark Streaming**

**–§–∞–π–ª:** `spark/streaming_job.py`

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç:**
1. –ß–∏—Ç–∞–µ—Ç —Å–æ–±—ã—Ç–∏—è –∏–∑ Kafka —Ç–æ–ø–∏–∫–∞ `telecom_events`
2. –ü–∞—Ä—Å–∏—Ç JSON –≤ DataFrame
3. –ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –ø–æ **1-–º–∏–Ω—É—Ç–Ω—ã–º –æ–∫–Ω–∞–º**:
   - –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞: `window`, `event_type`, `region`
   - –ú–µ—Ç—Ä–∏–∫–∏: `COUNT(*)`, `SUM(duration)`, `SUM(data_mb)`, `SUM(amount)`
4. –ö–∞–∂–¥—ã–µ **30 —Å–µ–∫—É–Ω–¥** –ø–∏—à–µ—Ç batch –≤ PostgreSQL (`real_time_metrics`)

**–í–æ–¥—è–Ω–æ–π –∑–Ω–∞–∫ (Watermark):**
- 2 –º–∏–Ω—É—Ç—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ late arrivals

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏:**
```python
.withWatermark("timestamp", "2 minutes")
.groupBy(window(col("timestamp"), "1 minute"), ...)
.trigger(processingTime="30 seconds")
```

---

### **4. Spark Batch**

**–§–∞–π–ª:** `spark/batch_job.py`

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç:**
1. –ß–∏—Ç–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ `real_time_metrics` –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—É—é –¥–∞—Ç—É
2. –ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç:
   - **–ü–æ —Ä–µ–≥–∏–æ–Ω–∞–º –∏ —Ç–∏–ø–∞–º —Å–æ–±—ã—Ç–∏–π**
   - **–û–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É** (region='ALL')
   - **–°–∞–º—ã–π –∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–≥–∏–æ–Ω** (event_type='most_active')
3. –ò–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–∞—è –∑–∞–ø–∏—Å—å –≤ `daily_stats` (DELETE + INSERT)

**–ó–∞–ø—É—Å–∫ –≤—Ä—É—á–Ω—É—é:**
```bash
docker exec spark-streaming /opt/spark/bin/spark-submit \
  --master local[2] \
  --packages org.postgresql:postgresql:42.7.1 \
  /app/batch_job.py 2025-11-09
```

**–ú–µ—Ç—Ä–∏–∫–∏:**
- `total_events` - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–±—ã—Ç–∏–π
- `total_duration_hours` - —Å—É–º–º–∞—Ä–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (—á–∞—Å—ã)
- `total_data_tb` - —Å—É–º–º–∞—Ä–Ω—ã–π —Ç—Ä–∞—Ñ–∏–∫ (—Ç–µ—Ä–∞–±–∞–π—Ç—ã)
- `total_amount` - —Å—É–º–º–∞—Ä–Ω—ã–µ –ø–æ–ø–æ–ª–Ω–µ–Ω–∏—è (—Ç–µ–Ω–≥–µ)
- `avg_duration_seconds` - —Å—Ä–µ–¥–Ω—è—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- `avg_data_per_user_mb` - —Å—Ä–µ–¥–Ω–∏–π —Ç—Ä–∞—Ñ–∏–∫ –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

---

### **5. PostgreSQL**

**–ë–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö:**
- `telecom_db` - –æ—Å–Ω–æ–≤–Ω–∞—è –ë–î
- `airflow_db` - –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ Airflow

**–¢–∞–±–ª–∏—Ü—ã:**

#### `real_time_metrics` (streaming —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã):
```sql
- window_start TIMESTAMP       # –ù–∞—á–∞–ª–æ –æ–∫–Ω–∞
- window_end TIMESTAMP         # –ö–æ–Ω–µ—Ü –æ–∫–Ω–∞
- event_type VARCHAR(50)       # –¢–∏–ø —Å–æ–±—ã—Ç–∏—è
- event_count INTEGER          # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–±—ã—Ç–∏–π
- total_duration BIGINT        # –°—É–º–º–∞—Ä–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (—Å–µ–∫)
- total_data_mb DOUBLE         # –°—É–º–º–∞—Ä–Ω—ã–π —Ç—Ä–∞—Ñ–∏–∫ (MB)
- total_amount DOUBLE          # –°—É–º–º–∞—Ä–Ω—ã–µ –ø–æ–ø–æ–ª–Ω–µ–Ω–∏—è
- region VARCHAR(100)          # –†–µ–≥–∏–æ–Ω
- processed_at TIMESTAMP       # –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
```

#### `daily_stats` (batch —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã):
```sql
- date DATE                    # –î–∞—Ç–∞
- event_type VARCHAR(50)       # –¢–∏–ø —Å–æ–±—ã—Ç–∏—è / 'most_active'
- region VARCHAR(100)          # –†–µ–≥–∏–æ–Ω / 'ALL'
- total_events BIGINT          # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–±—ã—Ç–∏–π
- total_duration_hours DOUBLE  # –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (—á–∞—Å—ã)
- total_data_tb DOUBLE         # –¢—Ä–∞—Ñ–∏–∫ (TB)
- total_amount DOUBLE          # –°—É–º–º–∞ –ø–æ–ø–æ–ª–Ω–µ–Ω–∏–π
- avg_duration_seconds DOUBLE  # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- avg_data_per_user_mb DOUBLE  # –°—Ä–µ–¥–Ω–∏–π —Ç—Ä–∞—Ñ–∏–∫
```

**–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ:**
```bash
psql -h localhost -p 5432 -U telecom_user -d telecom_db
```

---

### **6. Airflow**

**UI:** http://localhost:8080 (admin / admin)

**DAG:** `daily_metrics_batch`

**–†–∞—Å–ø–∏—Å–∞–Ω–∏–µ:** –ö–∞–∂–¥—ã–π –¥–µ–Ω—å –≤ 02:00 (`0 2 * * *`)

**–¢–∞—Å–∫–∏:**
```
check_data          # –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –∑–∞ –¥–∞—Ç—É
    ‚Üì
run_spark_batch     # –ó–∞–ø—É—Å–∫–∞–µ—Ç Spark batch job
    ‚Üì
validate_results    # –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–ø–∏—Å–∞–ª–∏—Å—å
    ‚Üì
notify              # –õ–æ–≥–∏—Ä—É–µ—Ç —É—Å–ø–µ—à–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
```

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –í —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–∏ `run_spark_batch` - –∑–∞–≥–ª—É—à–∫–∞. Batch –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –≤—Ä—É—á–Ω—É—é. –î–ª—è production –Ω—É–∂–µ–Ω `SparkSubmitOperator`.

---

## üêõ –ü—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è

### **1. Zookeeper: NodeExistsException**

**–ü—Ä–æ–±–ª–µ–º–∞:**
```
node already exists and owner does not match current session
```

Kafka –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è, –≤ Zookeeper –æ—Å—Ç–∞–ª–∏—Å—å "–≥—Ä—è–∑–Ω—ã–µ" –¥–∞–Ω–Ω—ã–µ.

**–†–µ—à–µ–Ω–∏–µ:**
```bash
docker-compose down -v  # –ß–∏—Å—Ç–∏–º volumes
docker-compose up -d
```

**–î–ª—è production:**
- –î–æ–±–∞–≤–∏—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–µ volumes –¥–ª—è Zookeeper
- –£–≤–µ–ª–∏—á–∏—Ç—å `KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS`
- –í—Å–µ–≥–¥–∞ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å —á–µ—Ä–µ–∑ `docker-compose down` (–Ω–µ `kill`)

---

### **2. Docker Desktop –ø–∞–¥–∞–µ—Ç**

**–ü—Ä–∏—á–∏–Ω–∞:** –ù–µ—Ö–≤–∞—Ç–∫–∞ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏ (7 –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ = ~3-4 GB)

**–†–µ—à–µ–Ω–∏–µ:**
1. Docker Desktop ‚Üí Settings ‚Üí Resources
2. Memory: **6-8 GB** (–º–∏–Ω–∏–º—É–º)
3. Swap: 2 GB
4. Restart Docker

**–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ:**
- –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –∫–æ–≥–¥–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è
- –†–µ–≥—É–ª—è—Ä–Ω–æ –æ—á–∏—â–∞—Ç—å: `docker system prune -a`
- –û—Ç–∫–ª—é—á–∏—Ç—å –∞–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ Docker Desktop


### **3. Airflow DAG –Ω–µ –ø–æ—è–≤–ª—è–µ—Ç—Å—è**

**–ü—Ä–∏—á–∏–Ω–∞:** Volume –Ω–µ –º–æ–Ω—Ç–∏—Ä—É–µ—Ç—Å—è –∏–ª–∏ scheduler –Ω–µ –ø–æ–¥—Ö–≤–∞—Ç–∏–ª —Ñ–∞–π–ª

**–†–µ—à–µ–Ω–∏–µ:**
```bash
# –ü—Ä–æ–≤–µ—Ä—å —á—Ç–æ —Ñ–∞–π–ª –≤–∏–¥–µ–Ω –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
docker exec airflow-scheduler ls /opt/airflow/dags/

# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏ scheduler
docker-compose restart airflow-scheduler

# –ü–æ–¥–æ–∂–¥–∏ 1-2 –º–∏–Ω—É—Ç—ã
```

### **4. Race condition —É Spark –º–æ–ª—á–∞ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç –≤—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –ø–æ—Å–ª–µ –ø–æ—è–≤–ª–µ–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤ —Å–∞–º –∑–∞–≤–µ—Ä—à–∞–µ—Ç**

**–ü—Ä–∏—á–∏–Ω–∞:** –ù–µ –±—ã–ª–æ master node –ø–æ—ç—Ç–æ–º—É docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ –±—ã–ª–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ 3 —è–¥—Ä–∞ 1 –¥–ª—è –º–∞—Å—Ç–µ—Ä –Ω–æ–¥—ã 

**–†–µ—à–µ–Ω–∏–µ:**

# –û—Å–≤–æ–±–∏—Ç—å —Ä–µ—Å—É—Ä—Å—ã
```

## üìä –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### **–ü—Ä–æ–≤–µ—Ä–∫–∞ real-time –º–µ—Ç—Ä–∏–∫:**

```sql
-- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–±—ã—Ç–∏–π –ø–æ —Ç–∏–ø–∞–º –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å
SELECT 
    event_type,
    COUNT(*) as windows,
    SUM(event_count) as total_events
FROM real_time_metrics
WHERE window_start >= NOW() - INTERVAL '1 hour'
GROUP BY event_type
ORDER BY total_events DESC;
```

### **–¢–æ–ø-5 —Ä–µ–≥–∏–æ–Ω–æ–≤ –ø–æ —Ç—Ä–∞—Ñ–∏–∫—É:**

```sql
SELECT 
    region,
    ROUND(SUM(total_data_mb)::numeric / 1024 / 1024, 4) as data_tb
FROM real_time_metrics
WHERE DATE(window_start) = CURRENT_DATE
GROUP BY region
ORDER BY data_tb DESC
LIMIT 5;
```

### **–î–Ω–µ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:**

```sql
SELECT 
    date,
    event_type,
    region,
    total_events,
    ROUND(total_data_tb::numeric, 4) as data_tb,
    ROUND(total_amount::numeric, 2) as amount
FROM daily_stats
WHERE date = '2025-11-09'
  AND region != 'ALL'
ORDER BY total_events DESC;
```

---

## üöÄ –î–∞–ª—å–Ω–µ–π—à–µ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ

### **–ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:**

- [ ] –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å `SparkSubmitOperator` –≤ Airflow
- [ ] –î–æ–±–∞–≤–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ (Prometheus + Grafana)
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∞–ª–µ—Ä—Ç—ã –Ω–∞ –∞–Ω–æ–º–∞–ª–∏–∏ (—Ä–µ–∑–∫–∏–µ —Å–∫–∞—á–∫–∏ —Ç—Ä–∞—Ñ–∏–∫–∞)
- [ ] –î–æ–±–∞–≤–∏—Ç—å CI/CD (GitHub Actions)
- [ ] Unit —Ç–µ—Å—Ç—ã –¥–ª—è Producer –∏ Spark jobs

### **–°—Ä–µ–¥–Ω–µ—Å—Ä–æ—á–Ω—ã–µ:**

- [ ] Kafka Connect –¥–ª—è —á—Ç–µ–Ω–∏—è –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
- [ ] Kafka Streams –¥–ª—è —Å–ª–æ–∂–Ω–æ–π event processing –ª–æ–≥–∏–∫–∏
- [ ] Delta Lake –¥–ª—è data lake —Å–ª–æ—è
- [ ] dbt –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ DWH
- [ ] Superset/Metabase –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏

### **–î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ:**

- [ ] Kubernetes deployment (Helm charts)
- [ ] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
- [ ] Multi-region —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—è
- [ ] Machine Learning –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç—Ç–æ–∫–∞
- [ ] Real-time —Ñ—Ä–æ–¥-–¥–µ—Ç–µ–∫—Ü–∏—è

---

## üìñ –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã

```bash
# –õ–æ–≥–∏ –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
docker-compose logs -f

# –õ–æ–≥–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞
docker-compose logs -f spark-streaming

# –†–µ—Å—Ç–∞—Ä—Ç —Å–µ—Ä–≤–∏—Å–∞
docker-compose restart kafka

# –°—Ç–∞—Ç—É—Å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
docker-compose ps

# –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤—Å—ë
docker-compose down

# –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å + —É–¥–∞–ª–∏—Ç—å volumes (–ü–û–¢–ï–†–Ø –î–ê–ù–ù–´–•!)
docker-compose down -v

# –ó–∞–π—Ç–∏ –≤ PostgreSQL
docker exec -it postgres psql -U telecom_user -d telecom_db

# –ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
docker stats --no-stream

# –û—á–∏—Å—Ç–∫–∞ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
docker system prune -a
```

