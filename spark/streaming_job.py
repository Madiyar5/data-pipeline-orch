from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    from_json, col, window, count, sum as spark_sum, 
    avg, current_timestamp, to_timestamp
)
from pyspark.sql.types import (
    StructType, StructField, StringType, 
    IntegerType, FloatType, TimestampType
)

print("=" * 60)
print("üöÄ –ó–∞–ø—É—Å–∫ Telecom Real-time Streaming Analytics")
print("=" * 60)

#spark session

spark = SparkSession.builder \
    .appName("TelecomStreamingAnalytics") \
    .config("spark.jars.packages", 
            "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,"
            "org.postgresql:postgresql:42.7.1") \
    .config("spark.sql.streaming.checkpointLocation", "/tmp/checkpoint") \
    .config("spark.sql.shuffle.partitions", "2") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

print("‚úÖ Spark Session —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ!")

#data

event_schema = StructType([
    StructField("event_id", StringType(), True),
    StructField("msisdn", StringType(), True),
    StructField("event_type", StringType(), True),
    StructField("event_subtype", StringType(), True),
    StructField("duration_seconds", IntegerType(), True),
    StructField("data_mb", FloatType(), True),
    StructField("amount", FloatType(), True),
    StructField("region", StringType(), True),
    StructField("cell_tower_id", IntegerType(), True),
    StructField("timestamp", StringType(), True)
])

print("‚úÖ –°—Ö–µ–º–∞ —Å–æ–±—ã—Ç–∏–π –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞")

#kafka

print("üîå –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Kafka...")

kafka_df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:29092") \
    .option("subscribe", "telecom_events") \
    .option("startingOffsets", "latest") \
    .option("failOnDataLoss", "false") \
    .load()

print("‚úÖ –ü–æ–¥–∫–ª—é—á–∏–ª–∏—Å—å –∫ Kafka —Ç–æ–ø–∏–∫—É 'telecom_events'")

#json parsing

events_df = kafka_df \
    .selectExpr("CAST(value AS STRING) as json") \
    .select(from_json(col("json"), event_schema).alias("data")) \
    .select("data.*") \
    .withColumn("timestamp", to_timestamp(col("timestamp")))

print("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ JSON —Å–æ–±—ã—Ç–∏–π –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

#aggregesion by windows

print("üìä –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∞–≥—Ä–µ–≥–∞—Ü–∏—é –ø–æ 1-–º–∏–Ω—É—Ç–Ω—ã–º –æ–∫–Ω–∞–º...")

aggregated_df = events_df \
    .withWatermark("timestamp", "2 minutes") \
    .groupBy(
        window(col("timestamp"), "1 minute"),
        col("event_type"),
        col("region")
    ) \
    .agg(
        count("*").alias("event_count"),
        spark_sum("duration_seconds").alias("total_duration"),
        spark_sum("data_mb").alias("total_data_mb"),
        spark_sum("amount").alias("total_amount")
    ) \
    .select(
        col("window.start").alias("window_start"),
        col("window.end").alias("window_end"),
        col("event_type"),
        col("event_count"),
        col("total_duration"),
        col("total_data_mb"),
        col("total_amount"),
        col("region"),
        current_timestamp().alias("processed_at")
    )

print("‚úÖ –ê–≥—Ä–µ–≥–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞: –æ–∫–Ω–∞ 1 –º–∏–Ω—É—Ç–∞, watermark 2 –º–∏–Ω—É—Ç—ã")

#write in postgresql 

jdbc_url = "jdbc:postgresql://postgres:5432/telecom_db"
db_properties = {
    "user": "telecom_user",
    "password": "telecom_pass",
    "driver": "org.postgresql.Driver"
}

def write_to_postgres(batch_df, batch_id):
    """
    batch –≤ PostgreSQL
    """
    if batch_df.count() == 0:
        print(f"‚ö†Ô∏è  Batch {batch_id}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–ø–∏—Å–∏")
        return
    
    try:
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º 
        print(f"\nüìù Batch {batch_id}: –∑–∞–ø–∏—Å—ã–≤–∞–µ–º {batch_df.count()} —Å—Ç—Ä–æ–∫")
        batch_df.show(5, truncate=False)
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ PostgreSQL
        batch_df.write \
            .jdbc(
                url=jdbc_url,
                table="real_time_metrics",
                mode="append",
                properties=db_properties
            )
        
        print(f"‚úÖ Batch {batch_id}: —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–Ω–æ –≤ PostgreSQL\n")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ batch {batch_id}: {e}\n")

print("üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ")

#streaming query

print("=" * 60)
print("üöÄ STREAMING QUERY –ó–ê–ü–£–©–ï–ù!")
print("=" * 60)
print("üìä –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–æ–±—ã—Ç–∏—è –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥")
print("üíæ –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ PostgreSQL")
print("‚è∞ –û–∫–Ω–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏: 1 –º–∏–Ω—É—Ç–∞")
print("üíß Watermark: 2 –º–∏–Ω—É—Ç—ã (–¥–ª—è late arrivals)")
print("\n–ù–∞–∂–º–∏ Ctrl+C –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏\n")
print("=" * 60)

query = aggregated_df \
    .writeStream \
    .outputMode("append") \
    .foreachBatch(write_to_postgres) \
    .trigger(processingTime="30 seconds") \
    .option("checkpointLocation", "/tmp/checkpoint") \
    .start()

# –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
try:
    query.awaitTermination()
except KeyboardInterrupt:
    print("\n\nüõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏...")
    query.stop()
    print("‚úÖ Streaming Query –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    spark.stop()
    print("‚úÖ Spark Session –∑–∞–∫—Ä—ã—Ç")