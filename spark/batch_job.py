from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum as spark_sum, avg, lit, row_number
from pyspark.sql.window import Window
import sys
import logging

# ==================== –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ====================
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==================== –ü–ê–†–ê–ú–ï–¢–†–´ ====================
if len(sys.argv) != 2:
    logger.error("Usage: spark-submit batch_job.py <processing_date>")
    sys.exit(1)

processing_date = sys.argv[1]  # –§–æ—Ä–º–∞—Ç: "2025-11-09"
logger.info(f"üöÄ Starting batch job for date: {processing_date}")

# ==================== SPARK SESSION ====================
spark = SparkSession.builder \
    .appName(f"DailySubscriberAggregation-{processing_date}") \
    .config("spark.jars.packages", 
            "org.postgresql:postgresql:42.7.1") \
    .config("spark.sql.shuffle.partitions", "2") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# ==================== JDBC –ü–ê–†–ê–ú–ï–¢–†–´ ====================
pg_url = "jdbc:postgresql://postgres:5432/telecom_db" 
pg_properties = {
    "user": "telecom_user",
    "password": "telecom_pass",
    "driver": "org.postgresql.Driver",
    "socketTimeout": "600",  # –¢–∞–π–º–∞—É—Ç —Å–æ–∫–µ—Ç–∞ (10 –º–∏–Ω—É—Ç)
    "loginTimeout": "60",    # –¢–∞–π–º–∞—É—Ç –≤—Ö–æ–¥–∞
    "sslmode": "disable"
}

# ==================== –ß–¢–ï–ù–ò–ï –î–ê–ù–ù–´–• ====================
logger.info("üìä Reading real-time metrics from PostgreSQL...")
query = f"""
    (SELECT * FROM real_time_metrics 
     WHERE DATE(window_start) = '{processing_date}') AS daily_metrics
"""

try:
    metrics_df = spark.read.jdbc(url=pg_url, table=query, properties=pg_properties)
    logger.warning("‚ö†Ô∏è  —Å—é–¥–∞ –∑–∞—à–µ–ª")
    if metrics_df.rdd.isEmpty():
        logger.warning(f"‚ö†Ô∏è  No metrics found for date {processing_date}")
        logger.warning("‚ö†Ô∏è  —Ç—ã –ª–æ—Ö")
        spark.stop()
        sys.exit(0)
    
    logger.info(f"‚úÖ Found {metrics_df.count()} records for {processing_date}")
    
except Exception as e:
    logger.error(f"‚ùå Error reading data: {e}")
    spark.stop()
    sys.exit(1)

# ==================== –ê–ì–†–ï–ì–ê–¶–ò–Ø –ü–û –†–ï–ì–ò–û–ù–ê–ú ====================
logger.info("üìà Aggregating metrics by region...")
region_metrics = metrics_df.groupBy("region", "event_type").agg(
    spark_sum("event_count").alias("total_events"),
    spark_sum("total_duration").alias("total_duration_seconds"),
    spark_sum("total_data_mb").alias("total_data_mb"),
    spark_sum("total_amount").alias("total_amount"),
    avg("total_duration").alias("avg_duration_seconds"),
    avg("total_data_mb").alias("avg_data_per_user_mb")
).withColumn("total_duration_hours", col("total_duration_seconds") / 3600) \
 .withColumn("total_data_tb", col("total_data_mb") / 1024 / 1024) \
 .withColumn("date", lit(processing_date).cast("date")) \
 .select(
     "date", "event_type", "region",
     "total_events", "total_duration_hours", "total_data_tb",
     "total_amount", "avg_duration_seconds", "avg_data_per_user_mb"
 )

# ==================== –û–ë–©–ò–ï –ú–ï–¢–†–ò–ö–ò ====================
logger.info("üìä Aggregating general metrics...")
general_metrics = metrics_df.groupBy("event_type").agg(
    spark_sum("event_count").alias("total_events"),
    spark_sum("total_duration").alias("total_duration_seconds"),
    spark_sum("total_data_mb").alias("total_data_mb"),
    spark_sum("total_amount").alias("total_amount"),
    avg("total_duration").alias("avg_duration_seconds"),
    avg("total_data_mb").alias("avg_data_per_user_mb")
).withColumn("total_duration_hours", col("total_duration_seconds") / 3600) \
 .withColumn("total_data_tb", col("total_data_mb") / 1024 / 1024) \
 .withColumn("region", lit("ALL")) \
 .withColumn("date", lit(processing_date).cast("date")) \
 .select(
     "date", "event_type", "region",
     "total_events", "total_duration_hours", "total_data_tb",
     "total_amount", "avg_duration_seconds", "avg_data_per_user_mb"
 )

# ==================== –°–ê–ú–´–ô –ê–ö–¢–ò–í–ù–´–ô –†–ï–ì–ò–û–ù ====================
logger.info("üèÜ Finding most active region...")
window_spec = Window.orderBy(col("total_events").desc())
active_region_df = region_metrics.withColumn("rank", row_number().over(window_spec)) \
    .filter(col("rank") == 1) \
    .select(
        lit(processing_date).cast("date").alias("date"),
        lit("most_active").alias("event_type"),
        col("region"),
        col("total_events"),
        col("total_duration_hours"),
        col("total_data_tb"),
        col("total_amount"),
        col("avg_duration_seconds"),
        col("avg_data_per_user_mb")
    )

# ==================== –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï ====================
final_df = region_metrics.union(general_metrics).union(active_region_df)

logger.info(f"üìù Total aggregated records: {final_df.count()}")

# ==================== –ò–î–ï–ú–ü–û–¢–ï–ù–¢–ù–ê–Ø –ó–ê–ü–ò–°–¨ ====================
logger.info("üíæ Writing results to daily_stats table...")

try:
    # –®–∞–≥ 1: –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞ —ç—Ç—É –¥–∞—Ç—É
    logger.info(f"üóëÔ∏è  Deleting old data for date {processing_date}...")
    delete_query = f"DELETE FROM daily_stats WHERE date = '{processing_date}'"
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º JDBC –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
    from py4j.java_gateway import java_import
    java_import(spark._jvm, "java.sql.DriverManager")
    
    conn = spark._jvm.DriverManager.getConnection(pg_url, pg_properties["user"], pg_properties["password"])
    stmt = conn.createStatement()
    deleted_rows = stmt.executeUpdate(delete_query)
    stmt.close()
    conn.close()
    
    logger.info(f"‚úÖ Deleted {deleted_rows} old records")

    # –®–∞–≥ 2: –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    final_df.repartition(1).write \
        .format("jdbc") \
        .option("url", pg_url) \
        .option("dbtable", "daily_stats") \
        .option("user", pg_properties["user"]) \
        .option("password", pg_properties["password"]) \
        .option("driver", pg_properties["driver"]) \
        .mode("append") \
        .save()
    
    logger.info(f"‚úÖ Successfully wrote {final_df.count()} records to daily_stats")
    
except Exception as e:
    logger.error(f"‚ùå Error writing data: {e}")
    spark.stop()
    sys.exit(1)

logger.info("üéâ Batch job completed successfully!")
spark.stop()